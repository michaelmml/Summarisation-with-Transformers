{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChineseEnglish.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOzMR1+dkrTdMNhMEaqZnRZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48tESPIaXEj4"
      },
      "outputs": [],
      "source": [
        "pip install -U tensorflow-text\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import logging\n",
        "import tensorflow_text as text\n",
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract training and validation datasets from WMT data. Transformer model with multi-headed self attention similar to summarisation task."
      ],
      "metadata": {
        "id": "HBoLYjTjZU5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = tfds.translate.wmt.WmtConfig(\n",
        "    description=\"WMT 2019 translation task dataset.\",\n",
        "    version=\"0.0.3\",\n",
        "    language_pair=(\"zh\", \"en\"),\n",
        "    subsets={\n",
        "        tfds.Split.TRAIN: [\"newscommentary_v14\"],\n",
        "        tfds.Split.VALIDATION: [\"newstest2018\"],\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "sullHzlZXrNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "q3_bPwgaXvtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pathlib\n",
        "root_folder='/content/drive/My Drive/ChEngTranslation'\n",
        "train_file = os.path.join(root_folder, \"train_samples\")\n",
        "test_file = os.path.join(root_folder, \"val_samples\")"
      ],
      "metadata": {
        "id": "5l7DNhQzXxLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder = tfds.builder(\"wmt_translate\", config=config)\n",
        "print(builder.info.splits)\n",
        "builder.download_and_prepare()\n",
        "datasets = builder.as_dataset(as_supervised=True)\n",
        "print('datasets is {}'.format(datasets))"
      ],
      "metadata": {
        "id": "8Iu4xZaXXsuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples = datasets[\"train\"]\n",
        "val_examples = datasets[\"validation\"]"
      ],
      "metadata": {
        "id": "Mz3YbysvXuI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.data.experimental.save(train_examples, train_file)\n",
        "tf.data.experimental.save(val_examples, test_file)"
      ],
      "metadata": {
        "id": "JrY9rp55Xybr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HuggingFace BERT tokenizer**"
      ],
      "metadata": {
        "id": "xvk84G7DZpIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer_en = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "tokenizer_zh = BertTokenizer.from_pretrained(\"bert-base-chinese\")"
      ],
      "metadata": {
        "id": "oHizeEyJXz5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def py_wrap_tokenize_pairs(en, zh):\n",
        "  return tf.numpy_function(tokenize_pairs, [en, zh], [tf.int64, tf.int64])\n",
        "\n",
        "def tokenize_pairs(en, zh):\n",
        "    en = tokenizer_en.tokenize(en.decode('utf-8'))\n",
        "    en = en.to_tensor()\n",
        "    zh = tokenizer_zh.tokenize(zh.decode('utf-8'))\n",
        "    zh = zh.to_tensor()\n",
        "    return en, zh"
      ],
      "metadata": {
        "id": "vMnn518PX25r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Alternative approach using SubwordTextEncoder**"
      ],
      "metadata": {
        "id": "HrnQelf8Zuz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en_vocab_file = os.path.join(root_folder, \"en_vocab\")\n",
        "\n",
        "subword_encoder_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (en.numpy() for en, _ in train_examples), \n",
        "    target_vocab_size=2**13)\n",
        "\n",
        "\n",
        "subword_encoder_en.save_to_file(en_vocab_file)"
      ],
      "metadata": {
        "id": "ANpyjIhZX6fT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zh_vocab_file = os.path.join(root_folder, \"zh_vocab\")\n",
        "\n",
        "subword_encoder_zh = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (zh.numpy() for _, zh in train_examples), \n",
        "    target_vocab_size=2**13)\n",
        "\n",
        "\n",
        "subword_encoder_zh.save_to_file(zh_vocab_file)"
      ],
      "metadata": {
        "id": "kpl3X4NzX_36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples = tf.data.experimental.load(train_file,element_spec=((tf.TensorSpec(shape=(), dtype=tf.string, name=None), tf.TensorSpec(shape=(), dtype=tf.string, name=None))))\n",
        "val_examples = tf.data.experimental.load(test_file,element_spec=((tf.TensorSpec(shape=(), dtype=tf.string, name=None), tf.TensorSpec(shape=(), dtype=tf.string, name=None))))"
      ],
      "metadata": {
        "id": "3GvJ0B3xYJT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_vocab_file = os.path.join(root_folder, \"en_vocab\")\n",
        "zh_vocab_file = os.path.join(root_folder, \"zh_vocab\")"
      ],
      "metadata": {
        "id": "v--OVenrYLf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subword_encoder_en = tfds.deprecated.text.SubwordTextEncoder.load_from_file(en_vocab_file)\n",
        "subword_encoder_zh = tfds.deprecated.text.SubwordTextEncoder.load_from_file(zh_vocab_file)"
      ],
      "metadata": {
        "id": "QkSPh928YPnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(en_t, zh_t):\n",
        "    en_indices = [subword_encoder_en.vocab_size] + subword_encoder_en.encode(\n",
        "      en_t.numpy()) + [subword_encoder_en.vocab_size + 1]\n",
        "\n",
        "    zh_indices = [subword_encoder_zh.vocab_size] + subword_encoder_zh.encode(\n",
        "      zh_t.numpy()) + [subword_encoder_zh.vocab_size + 1]\n",
        "\n",
        "    return en_indices, zh_indices"
      ],
      "metadata": {
        "id": "6rDdM2T5YQwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_t, zh_t = next(iter(train_examples))\n",
        "en_indices, zh_indices = encode(en_t, zh_t)"
      ],
      "metadata": {
        "id": "chH0KQbqYSIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"English subword vocab size：{subword_encoder_en.vocab_size}\")\n",
        "print(f\"Top 10 subwords：{subword_encoder_en.subwords[:10]}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "2ORJ-frBYUDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Chinese subword vocab size：{subword_encoder_zh.vocab_size}\")\n",
        "print(f\"Top 10 subwords：{subword_encoder_zh.subwords[:10]}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "k7jAJeXWYV1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_max_length(en, zh, max_length=MAX_LENGTH):\n",
        "    return tf.logical_and(tf.size(en) <= max_length,\n",
        "                        tf.size(zh) <= max_length)"
      ],
      "metadata": {
        "id": "4fO0BEq6YXMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 80\n",
        "BATCH_SIZE = 128\n",
        "BUFFER_SIZE = 15000\n",
        "\n",
        "train_dataset = (train_examples\n",
        "                 .map(tf_encode) \n",
        "                 .filter(filter_max_length)\n",
        "                 .cache()\n",
        "                 .shuffle(BUFFER_SIZE)\n",
        "                 .padded_batch(BATCH_SIZE,\n",
        "                               padded_shapes=([-1], [-1]))\n",
        "                 .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "val_dataset = (val_examples\n",
        "               .map(tf_encode)\n",
        "               .filter(filter_max_length)\n",
        "               .padded_batch(BATCH_SIZE, \n",
        "                             padded_shapes=([-1], [-1])))"
      ],
      "metadata": {
        "id": "Z-l0M8MYYcY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_batch, zh_batch = next(iter(train_dataset))"
      ],
      "metadata": {
        "id": "WYIh73wAYeL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Summarisationv2')"
      ],
      "metadata": {
        "id": "VQb92-4XYgs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import TransformerArchitecture\n",
        "from TransformerArchitecture import *"
      ],
      "metadata": {
        "id": "zMUBgPByYpet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 6\n",
        "d_model = 512\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "hidden_num = 10\n",
        "\n",
        "input_vocab_size = subword_encoder_en.vocab_size + 2\n",
        "target_vocab_size = subword_encoder_zh.vocab_size + 2\n",
        "dropout_rate = 0.3"
      ],
      "metadata": {
        "id": "Zu5lAkttYq4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "metadata": {
        "id": "qqDT0lVYYt7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "metadata": {
        "id": "i4dLzp_NYx-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "UReH9fGcY0zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ],
      "metadata": {
        "id": "Ct2flwfGY2hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=input_vocab_size,\n",
        "    target_vocab_size=target_vocab_size,\n",
        "    pe_input=1000,\n",
        "    pe_target=1000,\n",
        "    rate=dropout_rate)"
      ],
      "metadata": {
        "id": "wYQTTEBXY8PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "metadata": {
        "id": "k-nJd6rnY-yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_folder='/content/drive/My Drive/ChEngTranslation'\n",
        "checkpoint_path = os.path.abspath(os.path.join(root_folder))\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print('Latest checkpoint restored!!')"
      ],
      "metadata": {
        "id": "vLR7Fj-_ZAvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions, _ = transformer([inp, tar_inp],\n",
        "                                  training = True)\n",
        "      loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(accuracy_function(tar_real, predictions))"
      ],
      "metadata": {
        "id": "AA4ya4C-ZC0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "    train_step(inp, tar)\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  ckpt_save_path = ckpt_manager.save()\n",
        "  print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
      ],
      "metadata": {
        "id": "wQ6S2JzYZFXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Evaluate(tf.Module):\n",
        "  def __init__(self, subword_encoder_en, subword_encoder_zh, transformer):\n",
        "    self.subword_encoder_en = subword_encoder_en\n",
        "    self.subword_encoder_zh = subword_encoder_zh\n",
        "    self.transformer = transformer\n",
        "\n",
        "  def __call__(self, inp_sentence, max_length=100):\n",
        "    start_token = [self.subword_encoder_en.vocab_size]\n",
        "    end_token = [self.subword_encoder_en.vocab_size + 1]\n",
        "\n",
        "    inp_sentence = start_token + subword_encoder_en.encode(inp_sentence) + end_token\n",
        "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "\n",
        "    decoder_input = [subword_encoder_zh.vocab_size]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "    for i in range(max_length):\n",
        "\n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = self.transformer([encoder_input, \n",
        "                                                     output],\n",
        "                                                     False)\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == subword_encoder_zh.vocab_size+1:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights"
      ],
      "metadata": {
        "id": "LEHPh6pTZJtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention_weights(attention, sentence, result, layer):\n",
        "    fig = plt.figure(figsize=(16, 8))\n",
        "\n",
        "    sentence = subword_encoder_en.encode(sentence)\n",
        "\n",
        "    attention = tf.squeeze(attention[layer], axis=0)\n",
        "\n",
        "    for head in range(attention.shape[0]):\n",
        "        ax = fig.add_subplot(2, 4, head+1)\n",
        "\n",
        "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
        "\n",
        "        fontdict = {'fontsize': 10}\n",
        "\n",
        "        ax.set_xticks(range(len(sentence)+2))\n",
        "        ax.set_yticks(range(len(result)-1))\n",
        "\n",
        "        ax.set_ylim(len(result)-1.5, -0.5)\n",
        "\n",
        "        ax.set_xticklabels(\n",
        "            ['<start>']+[subword_encoder_en.decode([i]) for i in sentence]+['<end>'], \n",
        "            fontdict=fontdict, rotation=90)\n",
        "\n",
        "        ax.set_yticklabels([subword_encoder_zh.decode([i]) for i in result \n",
        "                            if i < subword_encoder_zh.vocab_size], \n",
        "                           fontdict=fontdict)\n",
        "\n",
        "        ax.set_xlabel('Head {}'.format(head+1))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "RS14rG4BZLkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence, plot):\n",
        "    result, attention_weights = evaluating(sentence)\n",
        "\n",
        "    predicted_sentence = subword_encoder_zh.decode([i for i in result \n",
        "                                            if i < subword_encoder_zh.vocab_size])  \n",
        "\n",
        "    print('Input: {}'.format(sentence))\n",
        "    print('Predicted translation: {}'.format(predicted_sentence))\n",
        "\n",
        "    if plot == True:\n",
        "        plot_attention_weights(attention_weights, sentence, result, 'decoder_layer4_block2')"
      ],
      "metadata": {
        "id": "nHSe64FRZOyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset_iters = iter(val_examples)"
      ],
      "metadata": {
        "id": "URgVrkAUZQMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zh_t, en_t = next(val_dataset_iters)\n",
        "translate(en_t.numpy().decode(\"utf-8\"), True)\n",
        "print (\"Real translation:\",zh_t.numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "id": "zIcu-EanZRkP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}